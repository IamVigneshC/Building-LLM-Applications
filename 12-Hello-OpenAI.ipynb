{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4772d59",
   "metadata": {},
   "source": [
    "# Hello World with OpenAI Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9128a04-4ba5-4762-a277-3e614725214b",
   "metadata": {},
   "source": [
    "Here we import the `OpenAI` library, which will enable us to interact with our locally hosted Llama 3.1 8b Instruct NIM, which exposes the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09eea9-c0d0-4962-a7d3-1d3f51b8573b",
   "metadata": {},
   "source": [
    "To start using the OpenAI API, we need to set up the OpenAI client. This involves configuring the base URL and providing an API key.\n",
    "\n",
    "By default, OpenAI API servers listen on port `8000` and expose the `/v1` endpoint. In our case, we have a NIM running locally on the same machine where you are interacting with this Jupyter environment, and the NIM is available at a host called `llama`. Therefore, to construct the `base_url` to interact with the NIM, we will use the `llama` hostname in conjunction with port `8000` and the `/v1` endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc5680-8e7c-4962-a33d-aa8cc5c60cb0",
   "metadata": {},
   "source": [
    "When creating an OpenAI client, the `api_key` argument is required, but in our case with the model running locally, we don't actually need to provide an API key. Therefore we will set the value of `api_key` to an arbitrary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f49d2c0-9279-4172-b441-a2d4df435b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'an_arbitrary_string'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a3cee-0beb-47b6-8442-3cebe57fe898",
   "metadata": {},
   "source": [
    "With a `base_url` and `api_key` we can now instantiate an OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b410e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=base_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce48a0-1348-4fd9-8bb1-cb3c05e3215a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca148f-e7db-4756-893e-6f2a2021b38d",
   "metadata": {},
   "source": [
    "## Observing Available Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48326ea-44b3-4b55-b625-844d98aec771",
   "metadata": {},
   "source": [
    "Now that we've created an OpenAI client, we can, as a first step, use it to observe any models available to us using a call to `client.models.list()`. In our case, as we've mentioned, we expect to see a Llama 3.1 8B Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f214e1-3a0a-4f80-aa13-6e44288a2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462496c6-5968-4835-ba60-d0ead7557104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='meta/llama-3.1-8b-instruct', created=1744931958, object='model', owned_by='system', root='meta/llama-3.1-8b-instruct', parent=None, max_model_len=131072, permission=[{'id': 'modelperm-a4dc0aaaa0234c79b0331682a4234237', 'object': 'model_permission', 'created': 1744931958, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}])], object='list')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a122bd-19c3-4447-ac28-5276e43b9ca9",
   "metadata": {},
   "source": [
    "There's a lot of information here that we are not concerned with, but if we drill into the object a little we can see more clearly the model we have available through the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7455ead1-bdc3-4ef1-91ee-74368495fc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta/llama-3.1-8b-instruct'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models.data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977ce91-76c6-4d9f-b091-c74b86c047e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0f44e-e634-44c7-b903-8bab3c196452",
   "metadata": {},
   "source": [
    "## Making a Simple Chat Completion Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb9743-ba83-4d62-a776-feeea2bda8fd",
   "metadata": {},
   "source": [
    "With the `client` instance now created, we can make a simple request to generate chat completions by using the `client.chat.completions.create` method which expects a `model` to use for the completion, as well as a list of `messages` to send to the model. We will be discussing the details of the `messages` list in more detail below, but for now we will pass in a simple single message containing a prompt from the user (you) asking for a fun fact about space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee8eb4e-b281-41fe-888e-2285d0669a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "prompt = 'Tell me a short fun fact about space.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f29d63-84bc-4a75-839f-f328507bfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b16139-d14f-46b6-9fc3-6a5f03c2a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chat-5cc6bd148df64894a3f7c96240e8653c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's one:\\n\\nDid you know that there is a giant storm on Jupiter that has been raging for at least 187 years? The Great Red Spot is a massive anticyclonic storm on Jupiter, which is larger than Earth in diameter, and has been continuously observed since 1831. It's still not understood how it's sustained for so long!\", refusal=None, role='assistant', function_call=None, tool_calls=None), stop_reason=None)], created=1744932263, model='meta/llama-3.1-8b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=73, prompt_tokens=21, total_tokens=94, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d62be-d6c2-4cb8-a1f8-cfad5960e2b0",
   "metadata": {},
   "source": [
    "There's a fair amount of information provided in the API response, but the part we are most interested in is the response from the model.\n",
    "\n",
    "Here we parse just the model's generated response out of the full API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d54e152d-a72e-4489-be51-6b495ecd139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd6ad10-476f-4e5e-bff0-0bdb01c9dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Did you know that there is a giant storm on Jupiter that has been raging for at least 187 years? The Great Red Spot is a massive anticyclonic storm on Jupiter, which is larger than Earth in diameter, and has been continuously observed since 1831. It's still not understood how it's sustained for so long!\n"
     ]
    }
   ],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f086f-7f1f-41e7-8189-3184b521872e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b8732-cabd-42a9-b826-99d73c177037",
   "metadata": {},
   "source": [
    "## Exercise: Create Your First Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82b4bd-fa06-4d16-a606-ce78daea60fc",
   "metadata": {},
   "source": [
    "Use our existing OpenAI API `client` to generate and print a response from our local Llama 3.1 8b model to a prompt of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5decfa36-bbf4-4cb8-9277-107f5c77ac67",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f737761e-6b06-4895-b2a3-e90220a06533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nvidia Neural IDM (NIM) platform is a powerful tool for developing and testing AI-powered applications, and it offers a range of features and tools for interacting with the platform. Here are some steps for interacting with Nvidia NIM:\n",
      "\n",
      "**Prerequisites**\n",
      "\n",
      "1. **Nvidia Account**: You need to have an Nvidia account to access NIM.\n",
      "2. **Nvidia NIM Configured**: Make sure you have NIM set up and configured on your system.\n",
      "3. **Code Editor or IDE**: Familiarize yourself with a code editor or IDE (Integrated Development Environment) such as Visual Studio Code, IntelliJ IDEA, or PyCharm.\n",
      "\n",
      "**Interacting with NIM**\n",
      "\n",
      "### 1. **Get Started with NIM**\n",
      "\n",
      "1.  Open a web browser and navigate to the NIM dashboard.\n",
      "2.  Log in with your Nvidia account credentials.\n",
      "3.  Create a new project or select an existing one.\n",
      "\n",
      "### 2. **Explore NIM Components**\n",
      "\n",
      "1. **Warp**: Warp is a cloud-based acceleration platform that enables you to run and manage AI workloads on GPUs.\n",
      "2. **Deep learning frameworks**: NIM provides support for popular deep learning frameworks like TensorFlow, PyTorch, and Keras, as well as other frameworks and libraries.\n",
      "3. **Managed servers and workspaces**: These allow you to create, manage, and destroy virtual environments for AI workloads.\n",
      "\n",
      "### 3. **Upload and Launch Models**\n",
      "\n",
      "1.  **Upload models**: Upload your model files to NIM.\n",
      "2.  **Create a job**: Create a job to run your model on the selected infrastructure.\n",
      "3.  **Launch the job**: Launch the job to execute the model on the GPU resources allocated by NIM.\n",
      "\n",
      "### 4. **Use NIM SDKs and APIs**\n",
      "\n",
      "Nvidia provides NIM SDKs and APIs for various programming languages, allowing you to programmatically interact with the platform. This lets you automate tasks, monitor performance, and create custom workflows.\n",
      "\n",
      "**Python Example: Working with NIM SDK**\n",
      "\n",
      "```python\n",
      "# Install the NIM SDK using pip:\n",
      "pip install nvidia-nim\n",
      "\n",
      "# Import the NIM SDK\n",
      "import sea-argro\n",
      "\n",
      "# Initialize the NIM client\n",
      "nim_client = SeaArgroClient()\n",
      "\n",
      "# List NIM projects\n",
      "projects = nim_client.projects.list(name=\"myproject\")\n",
      "print(projects)\n",
      "\n",
      "# Upload a model\n",
      "model_file = \"/path/to/model.h5\"\n",
      "nim_client.model.upload(model_file)\n",
      "\n",
      "# Launch a job\n",
      "job = nim_client.job.create(model_name=\"model_folder/model.h5\", input_file=\"input.txt\")\n",
      "nim_client.job.launch(job)\n",
      "```\n",
      "\n",
      "**Additional Tips and Considerations:**\n",
      "\n",
      "*   Follow the Nvidia documentation to get started with NIM.\n",
      "*   Take advantage of free services and test credits available on NIM.\n",
      "*   Be mindful of your NIM usage policies, including GPU and storage requirements.\n",
      "*   Read the documentation for the NIM SDKs and APIs to learn more about integrating NIM into your workflow.\n",
      "\n",
      "**Security Considerations**\n",
      "\n",
      "*   Avoid using unmodified images from untrusted sources to maintain the integrity of your AI models.\n",
      "*   Always use verifiable and authenticated code, including binaries, assets, and Docker images.\n",
      "*   Ensure that you remove data and stop NIM workloads when not in use to minimize unnecessary costs.\n",
      "\n",
      "The Nvidia Neural IDM (NIM) platform offers a powerful system development and integration environment for AI and computer vision workloads. This high-level guide to interacting with NIM provides a foundation for exploring NIM's vast feature set, from managed servers and workspaces to NIM SDKs and APIs.\n",
      "\n",
      "If you have further questions about using NIM or collaborative development for various innovations, feel free to ask, and we'll do our best to assist you!\n"
     ]
    }
   ],
   "source": [
    "prompt = 'How to interact with Nvidia NIM?'\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "model_response = response.choices[0].message.content\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bf0a3-b6ad-455d-a95e-a0cc9763225b",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d8bf54f-b631-45a1-b89c-c555f740e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What is the OpenAI API?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7c6cb76-7f78-47bf-ad1d-be363ee13727",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27550c33-309d-497d-b014-5806c942d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620b0031-8f17-4fda-884d-8116cdd45db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is a large language model developed by OpenAI, a privately held artificial intelligence research laboratory. The OpenAI API is an application programming interface (API) that allows developers to access and integrate the capabilities of the model into their own applications. Here's a general overview of what the OpenAI API can do:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Text generation**: The API can generate human-like text based on input prompts or topics. This can be used for text completion, chatbots, language translation, and more.\n",
      "2. **Content creation**: The API can generate content such as articles, stories, images, and code based on prompts or topics.\n",
      "3. **Conversational AI**: The API can be used to build conversational interfaces, such as chatbots, virtual assistants, and dialogue systems.\n",
      "4. **Content moderation**: The API can be used to detect and classify text for various purposes, such as language, sentiment, and toxicity analysis.\n",
      "5. **Recommendation systems**: The API can be used to generate personalized recommendations based on user behavior, preferences, and other factors.\n",
      "\n",
      "**Use cases:**\n",
      "\n",
      "1. **Customer service**: Integrate OpenAI-powered chatbots to automate customer support and answer frequently asked questions.\n",
      "2. **Language translation**: Use the API to translate text from one language to another in real-time.\n",
      "3. **Essay writing**: Generate articles, essays, or other written content using the API's text generation capabilities.\n",
      "4. **Content marketing**: Use the API to generate ideas for blog posts, product descriptions, or social media content.\n",
      "5. **Productivity tools**: Integrate the API into productivity tools, such as email clients, writing assistants, or scheduling apps.\n",
      "\n",
      "**API Endpoints:**\n",
      "\n",
      "The OpenAI API provides a set of endpoints that allow developers to interact with the model. Some of the key endpoints include:\n",
      "\n",
      "1. **/translate**: Translate text from one language to another.\n",
      "2. **/text-basics**: Perform text generation, text classification, and text completion tasks.\n",
      "3. **/completion**: Generate text based on a prompt or topic.\n",
      "4. **/ embryo**: Generate code or data based on a prompt or topic.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "While the OpenAI API is a powerful tool, it has some limitations:\n",
      "\n",
      "1. **Technical requirements**: The API requires a technical background and expertise in programming to use effectively.\n",
      "2. **Cost**: Some endpoints may require a paid subscription or usage limits.\n",
      "3. **Content ownership and usage**: Developers must acknowledge and respect the API's content usage policies, including intellectual property rights and data protection.\n",
      "4. **Error tolerance**: The model may produce incorrect or biased responses, as with any AI model.\n",
      "\n",
      "Overall, the OpenAI API is a powerful tool for developers and organizations looking to leverage AI-generated content and conversational interfaces in their applications.\n"
     ]
    }
   ],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a434ae7-0212-4dd7-95e3-09028e84412d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e582e-1dbf-411b-8611-46bfc681fc4d",
   "metadata": {},
   "source": [
    "## Understanding Completion and Chat Completion Endpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d468d177-3c90-46dc-9aa3-5b2b685c7619",
   "metadata": {},
   "source": [
    "We have been working with the `chat.completions` endpoint, but when working with the OpenAI API, you also have the option to use the `completions` endpoint. Understanding the differences between these endpoints is crucial, as they handle prompts and generate responses differently, even for a single prompt.\n",
    "\n",
    "The `chat.completions` endpoint is designed to handle multi-turn conversations, keeping track of the context provided by previous messages. It generates more concise, focused responses by anticipating a back-and-forth interaction, even if only a single prompt is provided.\n",
    "\n",
    "The `completions` endpoint is designed for generating a response to a single prompt without maintaining conversational context. It aims to complete the prompt that was given to it, rather than respond to it conversationally.\n",
    "\n",
    "The main takeaway is that when working with \"chat\" or \"instruction\" models (like the llama-3.1-8b-instruct model you are working with today), use `chat.completions` and not `completions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69074d67-d351-40a8-9c68-5c5f5edadd48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e117ca2-b3de-44fe-b2c6-79c651838ce7",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bbb6e-2948-4812-b902-e2bb9d3386ab",
   "metadata": {},
   "source": [
    "By completing this notebook, you should now have a basic understanding of how to use the OpenAI library to generate chat completions, and parse out the model response. This foundation will prepare you for more advanced topics and techniques in prompt engineering.\n",
    "\n",
    "In the next notebook, we will explore how to use LangChain to interact with language models, which will provide more flexibility and advanced capabilities for managing and generating text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
